import numpy as np

# Generate a sample sequence (e.g., to learn the pattern)
sequence_length = 10
sequence = np.sin(np.linspace(0, 3 * np.pi, sequence_length))  # Sine wave sequence

# Hyperparameters
input_size = 1         # One feature (sequence value at each step)
hidden_size = 10       # Size of the hidden layer
output_size = 1        # Predicting one value at a time
learning_rate = 0.001
epochs = 1000

# Initialize weights and biases
Wxh = np.random.randn(hidden_size, input_size) * 0.01  # Input to hidden
Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden to hidden
Why = np.random.randn(output_size, hidden_size) * 0.01  # Hidden to output
bh = np.zeros((hidden_size, 1))  # Hidden bias
by = np.zeros((output_size, 1))  # Output bias

# Sigmoid activation function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Derivative of sigmoid
def sigmoid_derivative(x):
    return x * (1 - x)

# Forward and backward pass through time
def train(sequence, Wxh, Whh, Why, bh, by, epochs, learning_rate):
    for epoch in range(epochs):
        h_prev = np.zeros((hidden_size, 1))  # Initialize hidden state
        total_loss = 0

        # Forward pass through the sequence
        for t in range(len(sequence) - 1):
            x = np.array([[sequence[t]]])  # Current input
            target = np.array([[sequence[t + 1]]])  # Next value in sequence (prediction target)

            # Compute hidden layer activation
            h_raw = np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh
            h = sigmoid(h_raw)

            # Compute output layer activation
            y = np.dot(Why, h) + by

            # Compute loss (Mean Squared Error)
            loss = (y - target) ** 2 / 2
            total_loss += loss

            # Backpropagation through time (BPTT)
            dL_dy = y - target
            dL_dWhy = np.dot(dL_dy, h.T)
            dL_dby = dL_dy

            dL_dh = np.dot(Why.T, dL_dy)
            dL_dh_raw = dL_dh * sigmoid_derivative(h)

            dL_dWxh = np.dot(dL_dh_raw, x.T)
            dL_dWhh = np.dot(dL_dh_raw, h_prev.T)
            dL_dbh = dL_dh_raw

            # Update weights and biases
            Wxh -= learning_rate * dL_dWxh
            Whh -= learning_rate * dL_dWhh
            Why -= learning_rate * dL_dWhy
            bh -= learning_rate * dL_dbh
            by -= learning_rate * dL_dby

            # Update hidden state for next time step
            h_prev = h

        if epoch % 100 == 0:
            print(f'Epoch {epoch}, Loss: {total_loss.item()}')

# Train the RNN on the sequence
train(sequence, Wxh, Whh, Why, bh, by, epochs, learning_rate)

# Test the trained RNN on a new sequence
h_prev = np.zeros((hidden_size, 1))
predicted_sequence = []
x = np.array([[sequence[0]]])  # Initial input

for t in range(len(sequence) - 1):
    h = sigmoid(np.dot(Wxh, x) + np.dot(Whh, h_prev) + bh)
    y = np.dot(Why, h) + by
    predicted_sequence.append(y.item())

    # Update for the next step
    x = np.array([[y]])  # Feed output as the next input
    h_prev = h

print("Predicted sequence:", predicted_sequence)
